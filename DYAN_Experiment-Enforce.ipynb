{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Import Section #################################\n",
    "\n",
    "## Imports related to PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "## Generic imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "\n",
    "## Dependencies classes and functions\n",
    "from utils import gridRing\n",
    "from utils import asMinutes\n",
    "from utils import timeSince\n",
    "from utils import getWeights\n",
    "from utils import save_checkpoint\n",
    "from utils import getListOfFolders\n",
    "\n",
    "## Import Model\n",
    "from DyanOF import OFModel,creatRealDictionary,fista\n",
    "\n",
    "############################# Import Section #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_dataset(model,size_dictionary,keys,device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cs = []\n",
    "        y_train = []\n",
    "        for key in keys:\n",
    "            X,y = size_dictionary[key]\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            T = X.shape[1]\n",
    "    #         c = model.forward(X,T).mean(-1)\n",
    "#             c = model.forward(X,T).view(-1,D*(N+1))\n",
    "    #         c,_ = torch.abs(model.forward(X,T)).max(-1)\n",
    "    #         c = torch.abs(model.forward(X,T)).sum(-1)\n",
    "#             c = model.encoder_.forward(X)\n",
    "            _,_,c = model.forward(X,T)\n",
    "    \n",
    "#             c = model.get_c(X,T)\n",
    "\n",
    "            cs.append(c.cpu().numpy())\n",
    "            y_train.extend(y.cpu().ravel().numpy())\n",
    "    X_train = np.vstack(cs)\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ilkay_dataset(model,size_dictionary,keys,device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        for key in keys:\n",
    "            X,Y = size_dictionary[key]\n",
    "            for i in range(X.shape[0]):\n",
    "                x = X[i].to(device)\n",
    "                y = Y[i].to(device)\n",
    "                T = x.shape[0]                \n",
    "                \n",
    "#                 embedding = model.encoder_.forward(x.unsqueeze(0)).squeeze(0)\n",
    "                _,_,embedding = model.forward(x.unsqueeze(0),T)\n",
    "                embedding.squeeze(0)\n",
    "\n",
    "#                 embedding = model.get_c(X,T)\n",
    "\n",
    "                x_train.append(embedding.cpu().numpy())\n",
    "                y_train.extend(y.cpu().ravel().numpy())\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_dataset_uneven(model,size_dictionary,keys,device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cs = {}\n",
    "        ys = {}\n",
    "        for key in keys:\n",
    "            X,y = size_dictionary[key]\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            T = X.shape[1]\n",
    "    #         c = model.forward(X,T).mean(-1)\n",
    "#             c = model.forward(X,T).view(-1,D*(N+1))\n",
    "    #         c,_ = torch.abs(model.forward(X,T)).max(-1)\n",
    "    #         c = torch.abs(model.forward(X,T)).sum(-1)\n",
    "#             c = model.encoder_.forward(X)\n",
    "            _,_,c = model.forward(X,T)\n",
    "#             c = model.get_c(X,T)\n",
    "\n",
    "\n",
    "            if cs.get(key) is None:\n",
    "                cs[key] = [c.cpu().numpy()]\n",
    "                ys[key] = deepcopy(y.cpu().ravel().numpy().tolist())\n",
    "            else:\n",
    "                cs[key].append(c.cpu().numpy())\n",
    "                ys[key].extend(y.cpu().ravel().numpy())\n",
    "    for key in keys:\n",
    "        cs[key] = np.vstack(cs[key])\n",
    "    return cs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].permute(1,0,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dde5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,D,embed_dim,latent_dim,nhead,encoder_layers=1,device='cuda:0'):\n",
    "        super(encoder,self).__init__()\n",
    "        self.D = D\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        self.input_projection = nn.Linear(D,embed_dim)\n",
    "        \n",
    "        self.tencoder = nn.ModuleList([nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1) for i in range(encoder_layers)])\n",
    "#         self.tencoder1 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "#         self.tencoder2 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim,latent_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         x = torch.tanh(self.input_projection(x))\n",
    "        x = self.pos_encoder(self.input_projection(x))\n",
    "        for encoder in self.tencoder:\n",
    "            x = torch.tanh(encoder(x))\n",
    "#         x = torch.tanh(self.tencoder1(x))\n",
    "#         x = torch.tanh(self.tencoder2(x))\n",
    "\n",
    "        latent = self.projection(x)\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dccd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,D,embed_dim,latent_dim,nhead,decoder_layers=1,device='cuda:0'):\n",
    "        super(decoder,self).__init__()\n",
    "        self.D = D\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        self.output_projection = nn.Linear(latent_dim,embed_dim)\n",
    "        \n",
    "        \n",
    "        self.tdecoder = nn.ModuleList([nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1) for i in range(decoder_layers)])\n",
    "\n",
    "#         self.tdecoder1 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "#         self.tdecoder2 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim,D)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         x = torch.tanh(self.output_projection(x))\n",
    "        x = self.pos_encoder(self.output_projection(x))\n",
    "        for decoder in self.tdecoder:\n",
    "            x = torch.tanh(decoder(x))\n",
    "#         x = torch.tanh(self.tdecoder1(x))\n",
    "#         x = torch.tanh(self.tdecoder2(x))\n",
    "\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDYANT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 Drr, \n",
    "                 Dtheta,\n",
    "                 N,\n",
    "                 D,\n",
    "                 embed_dim,\n",
    "                 latent_dim,\n",
    "                 nhead,\n",
    "                 N_class,\n",
    "                 encoder_layers=1,\n",
    "                 decoder_layers=1,\n",
    "                 device='cuda:0',\n",
    "                clamp=2):\n",
    "        super(TDYANT, self).__init__()\n",
    "        \n",
    "        self.rr = nn.Parameter(Drr)\n",
    "        self.theta = nn.Parameter(Dtheta)\n",
    "        #self.T = T\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        self.encoder_ = encoder(D,embed_dim,latent_dim,nhead,encoder_layers,device)\n",
    "        self.decoder_ = decoder(D,embed_dim,latent_dim,nhead,decoder_layers,device)\n",
    "        self.clamp = clamp\n",
    "    def forward(self, x,T):\n",
    "        if self.clamp == 0:\n",
    "            latent = self.encoder_(x)\n",
    "        else:\n",
    "            latent = torch.tanh(self.encoder_(x))*self.clamp #torch.clamp(self.encoder_(x),-2,2)\n",
    "#         latent = latent + torch.randn_like(latent)*np.sqrt(0.1)\n",
    "        dic = creatRealDictionary(T,self.rr,self.theta,self.device)\n",
    "        \n",
    "        ## for UCF Dataset:\n",
    "        # 0.1\n",
    "        sparsecode = fista(dic,latent,0.1,100,self.device)\n",
    "        y = torch.matmul(dic,sparsecode)\n",
    "        \n",
    "        ## for Kitti Dataset: sparsecode = fista(dic,x,0.01,80,self.gid)\n",
    "        \n",
    "        x = self.decoder_(y)\n",
    "        \n",
    "        # x is the outer layer , y is the inner layer\n",
    "        return x,latent,y\n",
    "    \n",
    "    def get_c(self,x,T):\n",
    "        if clamp == 0:\n",
    "            latent = self.encoder_(x)\n",
    "        else:\n",
    "            latent = torch.tanh(self.encoder_(x))*self.clamp\n",
    "        \n",
    "        dic = creatRealDictionary(T,self.rr,self.theta,self.device)\n",
    "        \n",
    "        ## for UCF Dataset:\n",
    "        sparsecode = fista(dic,latent,0.1,100,self.device)\n",
    "        \n",
    "        # x is the outer layer , y is the inner layer\n",
    "        return sparsecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dictionary(df,labels,chunk_size=1,task=\"classification\"):\n",
    "    size_dictionary = {}\n",
    "    for i in df.index.unique():\n",
    "        x = torch.FloatTensor(df.loc[i].values)\n",
    "#         x = (x - x.mean(0,keepdim=True))/x.std(0,keepdim=True)\n",
    "        size = x.shape[0]\n",
    "        if chunk_size > 1:\n",
    "            size = int(size/chunk_size)\n",
    "            \n",
    "        if task == \"classification\":\n",
    "            y = torch.LongTensor([labels.loc[i].item()])\n",
    "        else:\n",
    "            y = torch.FloatTensor([labels.loc[i].item()])\n",
    "            \n",
    "        if size_dictionary.get(size) is None:\n",
    "            if chunk_size > 1:\n",
    "                x = torch.chunk(x,chunk_size)\n",
    "                size_dictionary[size] = list(zip(deepcopy(x),[y]*chunk_size))\n",
    "            else:\n",
    "                size_dictionary[size] = [(x,y)]\n",
    "        else:\n",
    "            if chunk_size > 1:\n",
    "                x = torch.chunk(x,chunk_size)\n",
    "                y = [y]*chunk_size\n",
    "                size_dictionary[size].extend(list(zip(x,y)))\n",
    "            else:\n",
    "                size_dictionary[size].append((x,y))\n",
    "\n",
    "    for key in size_dictionary.keys():\n",
    "        Xs, ys = list(zip(*size_dictionary[key]))\n",
    "        size_dictionary[key] = (torch.stack(Xs,0),torch.stack(ys,0))\n",
    "    \n",
    "    keys = np.array(list(size_dictionary.keys()))[np.argsort([size_dictionary[key][0].shape[0] for key in size_dictionary.keys()])[::-1]]\n",
    "    return size_dictionary,keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dictionary_extended(df,labels):\n",
    "    MAX = np.max([df.loc[i].shape[0] for i in df.index.unique()])\n",
    "    size_dictionary = {}\n",
    "    for i in df.index.unique():\n",
    "        x = df.loc[i].values\n",
    "#         x = (x - x.mean(0,keepdim=True))/x.std(0,keepdim=True)\n",
    "\n",
    "        N_x = x.shape[0]\n",
    "        N_repeat = MAX-N_x\n",
    "        repeat_values = np.repeat(np.expand_dims(x[-1,:],0),[N_repeat],axis=0)\n",
    "        x = np.vstack((x,repeat_values))\n",
    "        x = torch.FloatTensor(x)\n",
    "        y = torch.LongTensor([labels.loc[i].item()])\n",
    "        if size_dictionary.get(MAX) is None:\n",
    "            size_dictionary[MAX] = [(x,y)]\n",
    "      \n",
    "        else:\n",
    "            size_dictionary[MAX].append((x,y))\n",
    "\n",
    "    for key in size_dictionary.keys():\n",
    "        Xs, ys = list(zip(*size_dictionary[key]))\n",
    "        size_dictionary[key] = (torch.stack(Xs,0),torch.stack(ys,0))\n",
    "    \n",
    "    keys = np.array(list(size_dictionary.keys()))[np.argsort([size_dictionary[key][0].shape[0] for key in size_dictionary.keys()])[::-1]]\n",
    "    return size_dictionary,keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,optimizer,criterion,size_dictionary,keys,lam1=1,lam2=1,lam3=1):\n",
    "    loss_value = []\n",
    "    loss1_value = []\n",
    "    loss2_value = []\n",
    "    norms_value = []\n",
    "    scheduler.step()\n",
    "    #for i_batch, sample in enumerate(dataloader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    model.train()\n",
    "    for key in keys:\n",
    "        Xs,ys = size_dictionary[key]\n",
    "        train_dl = DataLoader(TensorDataset(Xs,ys),batch_size=BATCH_SIZE,shuffle=True)\n",
    "        for X,y in train_dl:\n",
    "            T = X.shape[1]\n",
    "            X = X.to(device) #cuda()\n",
    "            y = y.to(device) #cuda()\n",
    "            data = X\n",
    "\n",
    "            inputData = Variable(data)\n",
    "            optimizer.zero_grad()\n",
    "            x_pred,latent,y_pred = model.forward(inputData,T)\n",
    "            \n",
    "\n",
    "            norms,MSE_LOSS1,MSE_LOSS2,loss = criterion(inputData,x_pred,latent,y_pred,lam1=lam1,lam2=lam2,lam3=lam3)\n",
    "\n",
    "            loss.backward()\n",
    "                        \n",
    "            optimizer.step()\n",
    "            loss_value.append(loss.data.item())\n",
    "            loss1_value.append(MSE_LOSS1.data.item())\n",
    "            loss2_value.append(MSE_LOSS2.data.item())\n",
    "            norms_value.append(norms.data.item())\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 predictions.extend(y_pred.argmax(1).cpu().numpy().tolist())\n",
    "#                 labels.extend(y.ravel().cpu().numpy().tolist())\n",
    "    \n",
    "    loss_val = np.mean(np.array(loss_value))\n",
    "    loss1_val = np.mean(np.array(loss1_value))\n",
    "    loss2_val = np.mean(np.array(loss2_value))\n",
    "    norms_val = np.mean(np.array(norms_value))\n",
    "    return (norms_val,loss1_val,loss2_val,loss_val),predictions,labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf74d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model,criterion,size_dictionary,keys,lam1=1,lam2=1,lam3=1):\n",
    "    with torch.no_grad():\n",
    "        loss_value = []\n",
    "        loss1_value = []\n",
    "        loss2_value = []\n",
    "        norms_value = []\n",
    "        #for i_batch, sample in enumerate(dataloader):\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        model.eval()\n",
    "        for key in keys:\n",
    "            Xs,ys = size_dictionary[key]\n",
    "            train_dl = DataLoader(TensorDataset(Xs,ys),batch_size=BATCH_SIZE,shuffle=True)\n",
    "            for X,y in train_dl:\n",
    "                T = X.shape[1]\n",
    "                X = X.to(device) #cuda()\n",
    "                y = y.to(device) #cuda()\n",
    "                data = X\n",
    "\n",
    "                inputData = Variable(data)\n",
    "\n",
    "                x_pred,latent,y_pred = model.forward(inputData,T)\n",
    "\n",
    "\n",
    "                norms,MSE_LOSS1,MSE_LOSS2,loss = criterion(inputData,x_pred,latent,y_pred,lam1=lam1,lam2=lam2,lam3=lam3)\n",
    "\n",
    "                loss_value.append(loss.data.item())\n",
    "                loss1_value.append(MSE_LOSS1.data.item())\n",
    "                loss2_value.append(MSE_LOSS2.data.item())\n",
    "                norms_value.append(norms.data.item())\n",
    "\n",
    "        loss_val = np.mean(np.array(loss_value))\n",
    "        loss1_val = np.mean(np.array(loss1_value))\n",
    "        loss2_val = np.mean(np.array(loss2_value))\n",
    "        norms_val = np.mean(np.array(norms_value))\n",
    "        return (norms_val,loss1_val,loss2_val,loss_val),predictions,labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x,x_pred,y,y_pred,lam1=1,lam2=1,lam3=1):\n",
    "    \"\"\"\n",
    "    lam1 is reconstruction loss\n",
    "    lam2 is latent reconstruction loss\n",
    "    lam3 is norm loss\n",
    "    \n",
    "    \"\"\"\n",
    "    MSE_LOSS_actual = torch.nn.functional.mse_loss(x_pred,x)\n",
    "    MSE_LOSS_latent = torch.nn.functional.mse_loss(y_pred,y)#\n",
    "    NORM_LOSS = torch.norm(y,p='fro',dim=1).mean()\n",
    "\n",
    "    loss = lam1*MSE_LOSS_actual + lam2*MSE_LOSS_latent - lam3*torch.clamp(NORM_LOSS,-1,1)  # if Kitti: loss = loss_mse(output, expectedOut)\n",
    "  \n",
    "#     with torch.no_grad():\n",
    "#         NORM_LOSS = torch.norm(y_pred,p='fro',dim=1).mean()\n",
    "\n",
    "    return NORM_LOSS,MSE_LOSS_latent,MSE_LOSS_actual,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HyperParameters for the Network\n",
    "NumOfPoles = 80\n",
    "\n",
    "N = NumOfPoles*4\n",
    "\n",
    "dataset_name = \"heartbeat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ebe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{dataset_name}/{dataset_name}_train_inputs.pickle\", \"rb\") as handle:\n",
    "    train_df = pickle.load(handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_train_labels.pickle\", \"rb\") as handle:\n",
    "    train_labels = pickle.load(handle) \n",
    "\n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_inputs.pickle\", \"rb\") as handle:\n",
    "    test_df = pickle.load(handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_labels.pickle\", \"rb\") as handle:\n",
    "    test_labels = pickle.load(handle)    \n",
    "    # train_dictionary,train_keys = create_data_dictionary(train_df,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.DataFrame(np.concatenate([np.expand_dims(train_df.loc[i],0) for i in np.unique(train_df.index)],axis=0))\n",
    "# test_df = pd.DataFrame(np.concatenate([np.expand_dims(test_df.loc[i],0) for i in np.unique(test_df.index)],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionary,test_keys = create_data_dictionary(test_df,test_labels,chunk_size=1,task=\"classification\")\n",
    "train_dictionary,train_keys = create_data_dictionary(train_df,train_labels,chunk_size=1,task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = train_df.shape[1]\n",
    "N_class = len(np.unique(train_labels.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0acbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Shape\",train_df.index.unique().shape)\n",
    "print(\"Testing Data Shape\",test_df.index.unique().shape)\n",
    "print(\"Training Data Classes\",N_class)\n",
    "print(\"Training Data Dimension: \",D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e207f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load saved model \n",
    "load_ckpt = False\n",
    "ckpt_file = 'preTrainedModel/UCFModel.pth' # for Kitti Dataset: 'KittiModel.pth'\n",
    "checkptname = dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing r, theta\n",
    "P,Pall = gridRing(N)\n",
    "Drr = abs(P)\n",
    "Drr = torch.from_numpy(Drr).float() #.to(device)\n",
    "Dtheta = np.angle(P)\n",
    "Dtheta = torch.from_numpy(Dtheta).float() #.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "latent_dim= 256\n",
    "nhead=16\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.0005\n",
    "EPOCH = 100\n",
    "print_every = 5\n",
    "saveEvery = 10\n",
    "clamp = 0\n",
    "encoder_layers=1\n",
    "decoder_layers=1\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model\n",
    "model = TDYANT(Drr,\n",
    "                Dtheta,\n",
    "                N ,\n",
    "                D, \n",
    "                embed_dim,\n",
    "                latent_dim,\n",
    "               nhead,\n",
    "                N_class,\n",
    "                encoder_layers,\n",
    "                decoder_layers,\n",
    "                device,\n",
    "                clamp).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=1e-8)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150,200], gamma=0.1) # if Kitti: milestones=[100,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device = \"cuda:0\"\n",
    "model = model.cuda()\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480cca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "## If want to continue training from a checkpoint\n",
    "if(load_ckpt):\n",
    "    loadedcheckpoint = torch.load(ckpt_file)\n",
    "    start_epoch = loadedcheckpoint['epoch']\n",
    "    model.load_state_dict(loadedcheckpoint['state_dict'])\n",
    "    optimizer.load_state_dict(loadedcheckpoint['optimizer'])\n",
    "\n",
    "print(\"Training from epoch: \", start_epoch)\n",
    "print('-' * 25)\n",
    "\n",
    "## Start the Training\n",
    "for epoch in range(start_epoch, EPOCH+1):\n",
    "    random.shuffle(train_keys)\n",
    "    start = time.time()\n",
    "    (norms,loss1,loss2,loss_val),predictions,labels = train_loop(model,optimizer,criterion,train_dictionary,train_keys,lam1=1,lam2=0.0,lam3=0)\n",
    "    end = time.time()\n",
    "    if (epoch)%print_every == 0:\n",
    "        (norms_v,loss1_v,loss2_v,loss_val_v),predictions,labels = evaluation_loop(model,criterion,test_dictionary,test_keys,lam1=1,lam2=0,lam3=0)\n",
    "\n",
    "    print('Epoch: ', epoch)\n",
    "    print(\"| train time: %.6f\" % (end-start))\n",
    "    print('| train loss: %.6f' % loss_val)\n",
    "    print('| train loss LATENT: %.6f' % loss1)\n",
    "    print('| train loss ACTUAL: %.6f' % loss2)\n",
    "    print('| LATENT NORM: %.6f' % norms)\n",
    "    if (epoch)%print_every == 0:\n",
    "        print('| val loss: %.6f' % loss_val_v)\n",
    "        print('| val loss LATENT: %.6f' % loss1_v)\n",
    "        print('| val loss ACTUAL: %.6f' % loss2_v)\n",
    "        print('| val LATENT NORM: %.6f' % norms_v)\n",
    "    print(\"\\n\")\n",
    "\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(labels,predictions,zero_division=1))\n",
    "    \n",
    "    if epoch % saveEvery ==0 :\n",
    "        print(\"Saving Checkpoint\")\n",
    "        save_checkpoint({'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict(),\n",
    "                        },f\"data/{checkptname}/\"+checkptname+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313da79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(False):\n",
    "    loadedcheckpoint = torch.load(r\"C:\\Users\\lpott\\Desktop\\DYAN\\Code\\data\\lorenz\\lorenz20.pth\")\n",
    "    start_epoch = loadedcheckpoint['epoch']\n",
    "    model.load_state_dict(loadedcheckpoint['state_dict'])\n",
    "    optimizer.load_state_dict(loadedcheckpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd147fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")\n",
    "model.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24858701",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    i=3\n",
    "    x_pred = model.forward(test_dictionary[T][0][n].unsqueeze(0).cuda(),T)[0].cpu().detach().numpy()\n",
    "    plt.figure(figsize=(10,10))\n",
    "#     for i in range(3):\n",
    "    plt.plot(np.arange(len(x_pred[0,:,i]))*0.01,test_dictionary[T][0][n,:,i])#,'b-')\n",
    "    plt.plot(np.arange(len(x_pred[0,:,i]))*0.01,x_pred[0,:,i])#,'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c657c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "I = 3\n",
    "T = 405\n",
    "with torch.no_grad():\n",
    "    # latent dim\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     embedding = torch.clamp(model.encoder_.forward(test_dictionary[T][0][n].unsqueeze(0).cuda()),-2,2)\n",
    "    if clamp == 0:\n",
    "        embedding = model.encoder_.forward(test_dictionary[T][0][n].unsqueeze(0).cuda())\n",
    "    else:\n",
    "        embedding = torch.tanh(model.encoder_.forward(test_dictionary[T][0][n].unsqueeze(0).cuda()))*clamp\n",
    "\n",
    "#     plt.plot(embedding[0].cpu().detach().numpy(),'b--')\n",
    "            \n",
    "    dic = creatRealDictionary(T,model.rr,model.theta,model.device)\n",
    "    sparsecode = fista(dic,embedding,0.1,100,model.device)\n",
    "    DYAN_embedding = torch.matmul(dic,sparsecode)[0].cpu().detach().numpy()\n",
    "#     plt.plot(DYAN_embedding,'r-')\n",
    "    \n",
    "    x_pred = model.forward(test_dictionary[T][0][n].unsqueeze(0).cuda(),T)[0].cpu().detach().numpy()\n",
    "    plt.figure(figsize=(10,10))\n",
    "#     for i in range(3):\n",
    "    plt.plot(np.arange(len(x_pred[0,:,:]))*0.01,test_dictionary[T][0][n,:,:])#,'b-')\n",
    "    plt.plot(np.arange(len(x_pred[0,:,:]))*0.01,x_pred[0,:,:])#,'r--')\n",
    "    plt.xlabel(\"Time (s)\",fontsize=20)\n",
    "    plt.ylabel(\"State\",fontsize=20)\n",
    "    plt.legend([\"x\",\"$x_{reconstructed}$\",\"y\",\"$y_{reconstructed}$\",\"z\",\"$z_{reconstructed}$\"],fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    nrow = 4; ncol = 8\n",
    "    fig, axs = plt.subplots(nrow, ncol)\n",
    "    if nrow == 1:\n",
    "        axs = np.expand_dims(axs,0)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            axs[i,j].plot(embedding[0,:,i*ncol + j].cpu().detach().numpy())\n",
    "            axs[i,j].plot(DYAN_embedding[:,i*ncol + j])\n",
    "            axs[i,j].title.set_text(f\"Feature {i*ncol+j+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9161274",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name ==\"nonlinear\" or \"UWaveGestureLibrary\":\n",
    "    plt.figure()\n",
    "    %matplotlib qt5\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(embedding[0,:,0].cpu().detach().numpy(),\n",
    "                 embedding[0,:,1].cpu().detach().numpy(),\n",
    "                 embedding[0,:,2].cpu().detach().numpy(),\n",
    "                 c=np.linspace(0,1,len(test_dictionary[T][0][0,:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(np.arange(0,10),np.arange(0,10),np.arange(0,10),c=np.arange(0,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'lorenz' or \"UWaveGestureLibrary\":\n",
    "    plt.figure()\n",
    "    %matplotlib qt5\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(test_dictionary[T][0][n,:,0],test_dictionary[T][0][n,:,1],test_dictionary[T][0][n,:,2],c=np.linspace(0,1,len(test_dictionary[T][0][0,:,0])))\n",
    "    ax.plot3D(x_pred[0,:,0],x_pred[0,:,1],x_pred[0,:,2])\n",
    "    ax.set_xlabel('$X$', fontsize=20)\n",
    "    ax.set_ylabel('$Y$',fontsize=20)\n",
    "    ax.set_zlabel(r'$Z$', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.figure()\n",
    "#     ax = plt.axes(projection='3d')\n",
    "#     ax.plot3D(DYAN_embedding[:,0],DYAN_embedding[:,1],DYAN_embedding[:,2])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9399b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_ahead = 2\n",
    "    embedding = torch.tanh(model.encoder_.forward(test_dictionary[T][0][n].unsqueeze(0).cuda()))*clamp\n",
    "    \n",
    "    dic = creatRealDictionary(T,model.rr,model.theta,model.device)\n",
    "    sparsecode = fista(dic,embedding,0.1,100,model.device)\n",
    "    dic_pred = creatRealDictionary(T+n_ahead,model.rr,model.theta,model.device)\n",
    "    DYAN_embedding = torch.matmul(dic_pred,sparsecode)#[0]\n",
    "    prediction = model.decoder_.forward(DYAN_embedding).cpu().detach().numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(np.arange(len(x_pred[0,:,:]))*0.01,test_dictionary[T][0][n,:,:])#,'b-')\n",
    "    plt.plot(0.01*T + np.arange(n_ahead)*0.01,test_dictionary[T][0][n+1,:n_ahead,:],'m*')\n",
    "    plt.plot(np.arange(len(prediction))*0.01,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77efa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = create_ilkay_dataset(model,train_dictionary,train_keys,device)\n",
    "X_test,y_test = create_ilkay_dataset(model,test_dictionary,test_keys,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{dataset_name}/{dataset_name}_train_inputs_ilkay.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_train,handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_train_labels_ilkay.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_train,handle)\n",
    "\n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_inputs_ilkay.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_test,handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_labels_ilkay.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_test,handle)\n",
    "    # train_dictionary,train_keys = create_data_dictionary(train_df,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device = \"cpu\"\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373bba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven = True if len(train_keys) > 1 else False \n",
    "if uneven:\n",
    "    X_train,y_train = create_numpy_dataset_uneven(model.to(\"cpu\"),train_dictionary,train_keys,device)\n",
    "    X_test,y_test = create_numpy_dataset_uneven(model.to(\"cpu\"),test_dictionary,test_keys,device)\n",
    "else:\n",
    "    X_train,y_train = create_numpy_dataset(model.to(\"cpu\"),train_dictionary,train_keys,device)\n",
    "    X_test,y_test = create_numpy_dataset(model.to(\"cpu\"),test_dictionary,test_keys,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c44ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{dataset_name}/{dataset_name}_train_inputs_concat.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_train,handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_train_labels_concat.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_train,handle)\n",
    "\n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_inputs_concat.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_test,handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_labels_concat.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_test,handle)\n",
    "    # train_dictionary,train_keys = create_data_dictionary(train_df,train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d8dc3",
   "metadata": {},
   "source": [
    "## PYTORCH CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self,D,N_class,embed_dim,latent_dim,num_heads=8,dim_feedforward=64):\n",
    "        super(classifier,self).__init__()\n",
    "    \n",
    "        \n",
    "        self.D = D\n",
    "        self.N_class = N_class\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        \n",
    "#         self.pos_encoder = PositionalEncoding(latent_dim)\n",
    "        \n",
    "        self.mha1 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=num_heads,batch_first=True,dim_feedforward=dim_feedforward,dropout=.1,norm_first=False,activation='relu')\n",
    "        self.mha2 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=num_heads,batch_first=True,dim_feedforward=dim_feedforward,dropout=.8)\n",
    "\n",
    "        self.project = nn.Linear(latent_dim,embed_dim)\n",
    "        self.l1 = nn.Linear(embed_dim*365,64)\n",
    "        self.decision = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         dic = creatRealDictionary(T,self.rr,self.theta,self.gid)\n",
    "#         ## for UCF Dataset:\n",
    "#         sparsecode = fista(dic,x,0.1,100,self.gid)\n",
    "        \n",
    "        x = torch.tanh(x)\n",
    "#         print(self.pos_encoder(torch.arange(x.size(1)).cuda()).shape)\n",
    "#         x = x + self.pos_encoder(torch.arange(x.size(1)).cuda()).unsqueeze(0)\n",
    "#         x = torch.tanh(self.pos_encoder(x))\n",
    "        x = torch.tanh(self.project(x))\n",
    "        x = torch.tanh(self.mha1(x))\n",
    "        x = torch.tanh(self.mha2(x))\n",
    "\n",
    "        x = x.reshape(-1,365*self.embed_dim)\n",
    "#         print(x.shape)\n",
    "        x= torch.tanh(self.l1(x))\n",
    "        x = torch.softmax(self.decision(x),-1)\n",
    "#         x = self.decision(x)\n",
    "        return x\n",
    "#mha_o = mha_o + c.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d377f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"UWaveGestureLibrary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92353e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{dataset_name}/{dataset_name}_train_inputs_concat.pickle\", \"rb\") as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_train_labels_concat.pickle\", \"rb\") as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "\n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_inputs_concat.pickle\", \"rb\") as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "    \n",
    "with open(f\"data/{dataset_name}/{dataset_name}_test_labels_concat.pickle\", \"rb\") as handle:\n",
    "    y_test = pickle.load(handle)\n",
    "    # train_dictionary,train_keys = create_data_dictionary(train_df,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(X_train) is dict:\n",
    "    X_train = np.concatenate(list(X_train.values()))\n",
    "    X_test = np.concatenate(list(X_test.values()))\n",
    "    y_train = np.concatenate(list(y_train.values()))\n",
    "    y_test = np.concatenate(list(y_test.values()))\n",
    "else:\n",
    "    X_train = torch.FloatTensor(X_train)#.permute(0,2,1)\n",
    "    X_test = torch.FloatTensor(X_test)#.permute(0,2,1)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89625c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train Shape\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[-1]\n",
    "N_class = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "model_c = classifier(D, N_class,32,3,8,64).cuda() #cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2576035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = (X_train-X_train.mean(dim=1,keepdim=True))/(X_train.std(dim=1,keepdim=True) + 1e-10)\n",
    "# X_test = (X_test-X_test.mean(dim=1,keepdim=True))/(X_test.std(dim=1,keepdim=True) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train.tolist(), return_counts=True)\n",
    "C = 1-torch.FloatTensor(counts/np.sum(counts)).to(device)\n",
    "# C = torch.FloatTensor([1,10]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9530c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = (X_train - X_train.mean(1,keepdim=True))/(X_train.std(1,keepdim=True)+1e-20)\n",
    "# X_test = (X_test - X_test.mean(1,keepdim=True))/(X_test.std(1,keepdim=True)+1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(TensorDataset(X_train,y_train),batch_size=64,shuffle=True)\n",
    "test_dl = DataLoader(TensorDataset(X_test,y_test),batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec438962",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()#nn.CrossEntropyLoss(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_c.parameters(),0.001,weight_decay=0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd50674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d8860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(400):\n",
    "    losses = []\n",
    "    model_c.train()\n",
    "    train_predictions = []\n",
    "    train_label_list = []\n",
    "    for x,y in train_dl:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        T = x.shape[1]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model_c(x)\n",
    "        \n",
    "        loss = criterion(y_pred.squeeze()*10,y)\n",
    "        loss.backward()\n",
    "        \n",
    "        losses.append(loss.item()*x.shape[0])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_predictions.extend(y_pred.ravel().tolist())#argmax(1).tolist())\n",
    "            train_label_list.extend(y.tolist())\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_c.eval()\n",
    "        test_predictions = []\n",
    "        for x,y in test_dl:\n",
    "            x = x.to(device)*10\n",
    "            y = y.to(device)\n",
    "            T = x.shape[1]\n",
    "            y_pred = model_c(x)\n",
    "            test_predictions.extend(y_pred.ravel().tolist())#.argmax(1).tolist())\n",
    "        print(\"=\"*10+f\"{epoch}\"+\"=\"*10)\n",
    "#         print(\"TRAIN\")\n",
    "        print(\"Train error: \",mean_squared_error(train_predictions,y_train))\n",
    "        print(\"Test error: \", mean_squared_error(test_predictions,y_test))\n",
    "    \n",
    "        print(classification_report(train_label_list,train_predictions,digits=4,zero_division=1))\n",
    "#         print(\"TEST\")\n",
    "        print(classification_report(y_test.tolist(),test_predictions,digits=4,zero_division=1))\n",
    "        accuracy = np.mean(np.array(y_test.tolist()) == np.array(test_predictions))\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy=accuracy\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ed237",
   "metadata": {},
   "source": [
    "## SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA,IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96693f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = train_dictionary[25] \n",
    "X_test,y_test = test_dictionary[25] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.numpy()\n",
    "y_train = y_train.numpy().ravel()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(X_train) is np.ndarray:\n",
    "    X_tr = X_train\n",
    "    X_te = X_test\n",
    "    y_te = y_test\n",
    "    y_tr= y_train\n",
    "else:\n",
    "    X_tr = X_train.cpu().numpy()\n",
    "    X_te = X_test.cpu().numpy()\n",
    "    y_te = y_test.cpu().numpy()\n",
    "    y_tr= y_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_tr.reshape(X_train.shape[0],-1)\n",
    "X_te = X_te.reshape(X_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751249c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e833bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_te.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eea98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=.995,random_state=0)\n",
    "svm = SVC(random_state=0,class_weight='balanced',kernel='linear',C=1,decision_function_shape='ovr')\n",
    "log = LogisticRegression(class_weight='balanced')\n",
    "# gpc = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# ada = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "# knn = KNeighborsClassifier(n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(pca,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f7a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = clf.predict(X_tr)\n",
    "pred_te = clf.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcdee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(pred_tr,y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(pred_te,y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_te,pred_te,'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_tr,pred_tr,'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa638bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,pred_tr,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea78edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_te,digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DYAN",
   "language": "python",
   "name": "dyan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
