{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Import Section #################################\n",
    "\n",
    "## Imports related to PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## Generic imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Dependencies classes and functions\n",
    "from utils import gridRing\n",
    "from utils import asMinutes\n",
    "from utils import timeSince\n",
    "from utils import getWeights\n",
    "from utils import save_checkpoint\n",
    "from utils import getListOfFolders\n",
    "\n",
    "## Import Model\n",
    "from DyanOF import OFModel,creatRealDictionary,fista\n",
    "\n",
    "############################# Import Section #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba95c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df,chunk_size=1):\n",
    "    X = []\n",
    "    for i in tqdm(df.index.unique()):\n",
    "        x = torch.FloatTensor(df.loc[i].values)\n",
    "        size = x.shape[0]\n",
    "        if chunk_size > 1:\n",
    "            size = int(size/chunk_size)\n",
    "        x = torch.chunk(x,chunk_size)\n",
    "        X.extend(x)\n",
    "    X = torch.stack(X, 0)\n",
    "    return X\n",
    "\n",
    "def load_dataset(dataset_name = \"lorenz\",file_path=r'C:\\Users\\lpott\\Desktop\\DYAN\\Code\\data',chunk_size=1):\n",
    "\n",
    "    with open(os.path.join(file_path,f\"{dataset_name}/{dataset_name}_train_inputs.pickle\"), \"rb\") as handle:\n",
    "        train_df = pickle.load(handle)\n",
    "\n",
    "    with open(os.path.join(file_path,f\"{dataset_name}/{dataset_name}_test_inputs.pickle\"), \"rb\") as handle:\n",
    "        test_df = pickle.load(handle)\n",
    "\n",
    "    X_train = load_data(train_df,chunk_size)\n",
    "    X_test = load_data(test_df,chunk_size)\n",
    "\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ba7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class differential_dataset(Dataset):\n",
    "\n",
    "    def __init__(self,X,horizon):\n",
    "\n",
    "        self.X = X\n",
    "        self.horizon = horizon\n",
    "        self.D = X.shape[-1]\n",
    "        self.T = X.shape[1]-self.horizon+1\n",
    "        \n",
    "        print(self.horizon)\n",
    "        print(self.T)\n",
    "#         self.mu = torch.mean(X,dim=[0,1])#torch.tensor([torch.mean(X[:,:,0]), torch.mean(X[:,:,1]), torch.mean(X[:,:,2])])\n",
    "#         self.std = torch.std(X,dim=[0,1])#torch.tensor([torch.std(X[:,:,0]), torch.std(X[:,:,1]), torch.std(X[:,:,2])])\n",
    "        self.mu = torch.tensor([torch.mean(X[:,:,i]) for i in range(self.D)])\n",
    "        self.std = torch.tensor([torch.std(X[:,:,i]) for i in range(self.D)])\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if type(idx) is int:\n",
    "            idx = [idx]\n",
    "\n",
    "        \n",
    "        start = torch.randint(low=0,high=self.T,size=(len(idx),))\n",
    "        windows = torch.tensor([list(range(i,i+self.horizon)) for i in start]).unsqueeze(-1).repeat(1,1,self.D)\n",
    "        x = torch.gather(self.X[idx],1,windows).squeeze()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].permute(1,0,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dde5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,D,embed_dim,latent_dim,dimforward,nhead,encoder_layers=1,device='cuda:0'):\n",
    "        super(encoder,self).__init__()\n",
    "        self.D = D\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        self.input_projection = nn.Linear(D,embed_dim)\n",
    "        \n",
    "        self.tencoder = nn.ModuleList([nn.TransformerEncoderLayer(d_model=embed_dim,dim_feedforward=dimforward,nhead=nhead,batch_first=True,dropout=0) for i in range(encoder_layers)])\n",
    "#         self.tencoder1 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "#         self.tencoder2 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim,latent_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim,dropout=0)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         x = torch.tanh(self.input_projection(x))\n",
    "        x = self.pos_encoder(self.input_projection(x))\n",
    "        sz = x.shape[1]\n",
    "#         mask = self.generate_square_subsequent_mask(sz)\n",
    "        for encoder in self.tencoder:\n",
    "            x = torch.tanh(encoder(x))\n",
    "#         x = torch.tanh(self.tencoder1(x))\n",
    "#         x = torch.tanh(self.tencoder2(x))\n",
    "\n",
    "        latent = self.projection(x)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def generate_square_subsequent_mask(self,sz):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dccd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,D,embed_dim,latent_dim,dimforward,nhead,decoder_layers=1,device='cuda:0'):\n",
    "        super(decoder,self).__init__()\n",
    "        self.D = D\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        self.output_projection = nn.Linear(latent_dim,embed_dim)\n",
    "        \n",
    "        \n",
    "        self.tdecoder = nn.ModuleList([nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,dim_feedforward=dimforward,batch_first=True,dropout=.0) for i in range(decoder_layers)])\n",
    "\n",
    "#         self.tdecoder1 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "#         self.tdecoder2 = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=nhead,batch_first=True,dropout=.1)\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim,D)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim,dropout=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         x = torch.tanh(self.output_projection(x))\n",
    "        x = self.pos_encoder(self.output_projection(x))\n",
    "        sz = x.shape[1]\n",
    "        mask = self.generate_square_subsequent_mask(sz)\n",
    "        for i,decoder in enumerate(self.tdecoder):\n",
    "            x = torch.tanh(decoder(x,mask))\n",
    "\n",
    "#         x = torch.tanh(self.tdecoder1(x))\n",
    "#         x = torch.tanh(self.tdecoder2(x))\n",
    "\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def generate_square_subsequent_mask(self,sz):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDYANT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 Drr, \n",
    "                 Dtheta,\n",
    "                 N,\n",
    "                 D,\n",
    "                 embed_dim,\n",
    "                 latent_dim,\n",
    "                 dimforward,\n",
    "                 nhead,\n",
    "                 encoder_layers=1,\n",
    "                 decoder_layers=1,\n",
    "                 device='cuda:0',\n",
    "                clamp=2):\n",
    "        super(TDYANT, self).__init__()\n",
    "        \n",
    "        self.rr = nn.Parameter(Drr)\n",
    "        self.theta = nn.Parameter(Dtheta)\n",
    "        #self.T = T\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dimforward = dimforward\n",
    "        \n",
    "        self.encoder_ = encoder(D,embed_dim,latent_dim,dimforward,nhead,encoder_layers,device)\n",
    "        self.decoder_ = decoder(D,embed_dim,latent_dim,dimforward,nhead,decoder_layers,device)\n",
    "        self.clamp = clamp\n",
    "        \n",
    "        self.register_buffer('mu', torch.zeros((D,)))\n",
    "        self.register_buffer('std', torch.ones((D,)))\n",
    "        \n",
    "    def forward(self, x,horizon):\n",
    "        x = self._normalize(x)\n",
    "        x_recon,x_ahead = x[:,:horizon,:],x[:,horizon:,:]\n",
    "        T = x_recon.shape[1]\n",
    "        T_Total = x_recon.shape[1] + x_ahead.shape[1]\n",
    "        \n",
    "        if self.clamp == 0:\n",
    "            latent = self.encoder_(x_recon)\n",
    "        else:\n",
    "            latent = torch.tanh(self.encoder_(x_recon))*self.clamp #torch.clamp(self.encoder_(x),-2,2)\n",
    "\n",
    "        dic = creatRealDictionary(T_Total,self.rr,self.theta,device=self.device)\n",
    "        dic_recon = dic[:horizon]\n",
    "#         dic_ahead = dic[horizon:]\n",
    "        \n",
    "        ## for UCF Dataset:\n",
    "        # 0.1\n",
    "        sparsecode = fista(dic_recon,latent,0.01,100,self.device)\n",
    "#         y_recon = torch.matmul(dic_recon,sparsecode)\n",
    "        \n",
    "#         dic = creatRealDictionary(T_Total,self.rr,self.theta,self.device)\n",
    "#         y_ahead = torch.matmul(dic_ahead,sparsecode) #[:,horizon:,:]\n",
    "        y = torch.matmul(dic,sparsecode)\n",
    "        y_recon = y[:,:horizon,:]\n",
    "        y_ahead = y[:,horizon:,:]\n",
    "        ## for Kitti Dataset: sparsecode = fista(dic,x,0.01,80,self.gid)\n",
    "        \n",
    "        x_recon_all = self.decoder_(y)\n",
    "        x_recon_hat = x_recon_all[:,:horizon,:]\n",
    "        x_ahead_hat = x_recon_all[:,horizon:,:]\n",
    "\n",
    "#         x_recon_hat = self.decoder_(y_recon)\n",
    "#         x_ahead_hat = self.decoder_(y_ahead)\n",
    "\n",
    "        # x is the outer layer , y is the inner layer\n",
    "        x_recon_hat = self._unnormalize(x_recon_hat)\n",
    "        x_ahead_hat = self._unnormalize(x_ahead_hat)\n",
    "\n",
    "        return x_recon_hat,x_ahead_hat,latent,y_recon,y_ahead\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        return (x - self.mu.unsqueeze(0).unsqueeze(0))/self.std.unsqueeze(0).unsqueeze(0)    \n",
    "    \n",
    "    def _unnormalize(self, x):\n",
    "        return self.std.unsqueeze(0).unsqueeze(0)*x + self.mu.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_loss(x,model,horizon,alpha=1):\n",
    "    x_recon,x_ahead = x[:,:horizon,:],x[:,horizon:,:]\n",
    "    x_recon_hat,x_ahead_hat,latent,y_recon,y_ahead = model(x,horizon)\n",
    "    \n",
    "    reconstruction_loss = F.mse_loss(x_recon_hat,x_recon)\n",
    "    prediction_loss = F.mse_loss(x_ahead_hat,x_ahead)\n",
    "    \n",
    "    loss = reconstruction_loss + alpha*prediction_loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        MSE_LOSS_latent = torch.nn.functional.mse_loss(y_recon,latent)#\n",
    "        NORM_LOSS = torch.norm(y_recon,p='fro',dim=2).mean()\n",
    "    \n",
    "    return loss,reconstruction_loss,prediction_loss,MSE_LOSS_latent,NORM_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HyperParameters for the Network\n",
    "NumOfPoles = 60\n",
    "\n",
    "N = NumOfPoles*4\n",
    "\n",
    "Time_Length = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e207f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load saved model \n",
    "load_ckpt = False\n",
    "checkptname = \"lorenz_prediction_short\" #\"lorenz_prediction\\lorenz_prediction50_best\"\n",
    "dataset_name = \"lorenz\"\n",
    "file_path=r'C:\\Users\\lpott\\Desktop\\DYAN\\Code\\data'\n",
    "ckpt_file = f\"data/{dataset_name}/\"+checkptname+str(170)+'.pth' # for Kitti Dataset: 'KittiModel.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f12023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = load_dataset(dataset_name=dataset_name,file_path=file_path,chunk_size=1)\n",
    "print(\"X_tr shape: \",X_train.shape)\n",
    "print(\"X_te shape: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing r, theta\n",
    "P,Pall = gridRing(N)\n",
    "Drr = abs(P)\n",
    "Drr = torch.from_numpy(Drr).float() #.to(device)\n",
    "Dtheta = np.angle(P)\n",
    "Dtheta = torch.from_numpy(Dtheta).float() #.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "embed_dim = 256\n",
    "latent_dim= 128\n",
    "dimfeedforward = 1024\n",
    "nhead=32\n",
    "D = X_train.shape[-1]\n",
    "clamp = 0\n",
    "encoder_layers=1\n",
    "decoder_layers=2\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LR = 0.0001\n",
    "EPOCH = 300\n",
    "print_every = 5\n",
    "saveEvery = 10\n",
    "\n",
    "# TIME PARAMETERS\n",
    "Time_Length = Time_Length\n",
    "horizon = 16\n",
    "alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(differential_dataset(X_train,Time_Length),batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dl = DataLoader(differential_dataset(X_test,Time_Length),batch_size=BATCH_SIZE)\n",
    "test_evaluator_dl = DataLoader(TensorDataset(X_test[:,-Time_Length:,:]),batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dl:\n",
    "    print(i.shape)\n",
    "    break\n",
    "plt.plot(i[0,:,0])\n",
    "plt.plot(i[0,:,1])\n",
    "# plt.plot(i[0,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model\n",
    "model = TDYANT(Drr,\n",
    "                Dtheta,\n",
    "                N ,\n",
    "                D, \n",
    "                embed_dim,\n",
    "                latent_dim,\n",
    "               dimfeedforward,\n",
    "               nhead,\n",
    "                encoder_layers,\n",
    "                decoder_layers,\n",
    "                device,\n",
    "                clamp).to(device)\n",
    "\n",
    "model.mu = train_dl.dataset.mu.to(device)\n",
    "model.std = train_dl.dataset.std.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=1e-8)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150,200], gamma=0.1) # if Kitti: milestones=[100,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device = \"cuda:0\"\n",
    "model = model.cuda()\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "start_epoch = 1\n",
    "\n",
    "## If want to continue training from a checkpoint\n",
    "if(load_ckpt):\n",
    "    print(\"LOADING CHECKPT\")\n",
    "    loadedcheckpoint = torch.load(ckpt_file)\n",
    "    start_epoch = loadedcheckpoint['epoch']\n",
    "    model.load_state_dict(loadedcheckpoint['state_dict'])\n",
    "    optimizer.load_state_dict(loadedcheckpoint['optimizer'])\n",
    "\n",
    "print(\"Training from epoch: \", start_epoch)\n",
    "print('-' * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480cca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Start the Training\n",
    "for epoch in range(start_epoch, EPOCH+1):\n",
    "   \n",
    "    model.train()\n",
    "    \n",
    "    train_epoch_loss = []; test_epoch_loss = []\n",
    "    train_norm_loss = []; test_norm_loss = []\n",
    "    train_latent_loss = []; test_latent_loss = [];\n",
    "    train_reconstruction_loss = []; test_reconstruction_loss = [];\n",
    "    train_prediction_loss = []; test_prediction_loss = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for x in tqdm(train_dl):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss,reconstruction_loss,prediction_loss,latent_loss,norm_loss = dynamic_loss(x,model,horizon,alpha=alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss.append(loss.item()*x.shape[0])\n",
    "        train_latent_loss.append(latent_loss.item()*x.shape[0])\n",
    "        train_norm_loss.append(norm_loss.item()*x.shape[0])\n",
    "        train_reconstruction_loss.append(reconstruction_loss.item()*x.shape[0])\n",
    "        train_prediction_loss.append(prediction_loss.item()*x.shape[0])\n",
    "        end = time.time()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (epoch)%print_every == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x in tqdm(test_evaluator_dl):\n",
    "                x = x[0].to(device)\n",
    "#                 x = X_test[:,-72:,:].to(device)\n",
    "                loss,reconstruction_loss,prediction_loss,latent_loss,norm_loss = dynamic_loss(x,model,horizon,alpha=alpha)\n",
    "                test_epoch_loss.append(loss.item()*x.shape[0])\n",
    "                test_latent_loss.append(latent_loss.item()*x.shape[0])\n",
    "                test_norm_loss.append(norm_loss.item()*x.shape[0])\n",
    "                test_reconstruction_loss.append(reconstruction_loss.item()*x.shape[0])\n",
    "                test_prediction_loss.append(prediction_loss.item()*x.shape[0])\n",
    "                \n",
    "    print('Epoch: ', epoch)\n",
    "    print(\"| train time: %.6f\" % (end-start))\n",
    "    print('| train loss: %.6f' % (np.sum(train_epoch_loss)/N_train))\n",
    "    print('| train reconstruction loss: %.6f' % (np.sum(train_reconstruction_loss)/N_train))\n",
    "    print('| train prediction loss: %.6f' % (np.sum(train_prediction_loss)/N_train))\n",
    "    print('| train LATENT loss: %.6f' % (np.sum(train_latent_loss)/N_train))\n",
    "    print('| LATENT NORM: %.6f' % (np.sum(train_norm_loss)/N_train))\n",
    "    if (epoch)%print_every == 0:\n",
    "        print('| val loss: %.6f' % (np.sum(test_epoch_loss)/N_test))\n",
    "        print('| val reconstruction loss: %.6f' % (np.sum(test_reconstruction_loss)/N_test))\n",
    "        print('| val prediction loss: %.6f' % (np.sum(test_prediction_loss)/N_test))\n",
    "        print('| val LATENT loss: %.6f' % (np.sum(test_latent_loss)/N_test))\n",
    "        print('| val LATENT NORM: %.6f' % (np.sum(test_norm_loss)/N_test))\n",
    "    print(\"\\n\")\n",
    "\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(labels,predictions,zero_division=1))\n",
    "    \n",
    "    if (epoch+1) % saveEvery ==0 :\n",
    "        print(\"Saving Checkpoint\")\n",
    "        save_checkpoint({'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict(),\n",
    "                        },f\"data/{dataset_name}/\"+checkptname+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Saving Checkpoint\")\n",
    "# checkptname = \"nonlinear_prediction_75\" #\"lorenz_prediction\\lorenz_prediction50_best\"\n",
    "# save_checkpoint({'epoch': epoch + 1,\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 'optimizer' : optimizer.state_dict(),\n",
    "#                 },f\"data/{dataset_name}/\"+checkptname+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96518df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## If want to continue training from a checkpoint\n",
    "ckpt_file = f\"data/{dataset_name}/\"+\"lorenz_prediction\"+str(250)+'.pth' # for Kitti Dataset: 'KittiModel.pth'\n",
    "# checkptname = \"lorenz_prediction\\lorenz_400_best_noclamp85.pth\"\n",
    "if(True):\n",
    "    print(\"LOADING CHECKPT\")\n",
    "    loadedcheckpoint = torch.load(ckpt_file)\n",
    "    start_epoch = loadedcheckpoint['epoch']\n",
    "    model.load_state_dict(loadedcheckpoint['state_dict'])\n",
    "    optimizer.load_state_dict(loadedcheckpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb980f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_epoch_loss = []; test_epoch_loss = []\n",
    "    train_norm_loss = []; test_norm_loss = []\n",
    "    train_latent_loss = []; test_latent_loss = [];\n",
    "    train_reconstruction_loss = []; test_reconstruction_loss = [];\n",
    "    train_prediction_loss = []; test_prediction_loss = []\n",
    "    for x in tqdm(test_evaluator_dl):\n",
    "        x = x[0].to(device)\n",
    "        loss,reconstruction_loss,prediction_loss,latent_loss,norm_loss = dynamic_loss(x,model,horizon,alpha=alpha)\n",
    "        test_epoch_loss.append(loss.item()*x.shape[0])\n",
    "        test_latent_loss.append(latent_loss.item()*x.shape[0])\n",
    "        test_norm_loss.append(norm_loss.item()*x.shape[0])\n",
    "        test_reconstruction_loss.append(reconstruction_loss.item()*x.shape[0])\n",
    "        test_prediction_loss.append(prediction_loss.item()*x.shape[0])\n",
    "print('| val loss: %.6f' % (np.sum(test_epoch_loss)/N_test))\n",
    "print('| val reconstruction loss: %.6f' % (np.sum(test_reconstruction_loss)/N_test))\n",
    "print('| val prediction loss: %.6f' % (np.sum(test_prediction_loss)/N_test))\n",
    "print('| val LATENT loss: %.6f' % (np.sum(test_latent_loss)/N_test))\n",
    "print('| val LATENT NORM: %.6f' % (np.sum(test_norm_loss)/N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss,reconstruction_loss,prediction_loss,latent_loss,norm_loss = dynamic_loss(X_test[:,-Time_Length:,:].to(device),model,horizon,alpha=alpha)\n",
    "    print('| val loss: %.6f' % (loss))\n",
    "    print('| val reconstruction loss: %.6f' % (reconstruction_loss))\n",
    "    print('| val prediction loss: %.6f' % (prediction_loss))\n",
    "    print('| val LATENT loss: %.6f' % (latent_loss))\n",
    "    print('| val LATENT NORM: %.6f' % (norm_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")\n",
    "model.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c657c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "x = X_test[[n],-Time_Length:,:]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon_hat,x_ahead_hat,latent,y_recon,y_ahead = model(x.to(device),horizon)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "#     for i in range(3):\n",
    "    plt.plot(np.arange(Time_Length),x[0])\n",
    "    plt.plot(np.arange(horizon),x_recon_hat[0,:,:].cpu(),'--')\n",
    "    plt.plot(horizon+np.arange(Time_Length-horizon),x_ahead_hat[0,:,:].cpu(),'r.')\n",
    "\n",
    "    plt.xlabel(\"Time (n)\",fontsize=20)\n",
    "    plt.ylabel(\"State\",fontsize=20)\n",
    "    plt.legend([\"x\",\"y\",\"z\",\"$x_{reconstructed}$\",\"$y_{reconstructed}$\",\"$z_{reconstructed}$\",\"Prediction\"],fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    nrow = 8; ncol = 8\n",
    "    fig, axs = plt.subplots(nrow, ncol)\n",
    "    if nrow == 1:\n",
    "        axs = np.expand_dims(axs,0)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            axs[i,j].plot(latent[0,:,i*ncol + j].cpu().detach().numpy())\n",
    "            axs[i,j].plot(y_recon[0,:,i*ncol + j].cpu().detach().numpy())\n",
    "            axs[i,j].title.set_text(f\"Feature {i*ncol+j+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ae7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    nrow = 8; ncol = 8\n",
    "    fig, axs = plt.subplots(nrow, ncol)\n",
    "    if nrow == 1:\n",
    "        axs = np.expand_dims(axs,0)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            axs[i,j].plot(y_ahead[0,:,i*ncol + j].cpu().detach().numpy())\n",
    "            axs[i,j].title.set_text(f\"Feature {i*ncol+j+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    nrow = 8; ncol = 8\n",
    "    fig, axs = plt.subplots(nrow, ncol)\n",
    "    if nrow == 1:\n",
    "        axs = np.expand_dims(axs,0)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            axs[i,j].plot(torch.concat((y_recon[0,:,i*ncol + j],y_ahead[0,:,i*ncol + j])).cpu().detach().numpy())\n",
    "            axs[i,j].title.set_text(f\"Feature {i*ncol+j+1}\")\n",
    "            axs[i,j].title.set_fontsize(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "%matplotlib qt5\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x[0,:,0],x[0,:,1],x[0,:,2],'k-') #c=np.linspace(0,1,Time_Length))\n",
    "ax.set_xlabel('$X$', fontsize=20)\n",
    "ax.set_ylabel('$Y$',fontsize=20)\n",
    "ax.set_zlabel(r'$Z$', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e15b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "%matplotlib qt5\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x[0,:,0],x[0,:,1],x[0,:,2],'k-') #c=np.linspace(0,1,Time_Length))\n",
    "ax.plot3D(x_recon_hat[0,:,0].cpu(),x_recon_hat[0,:,1].cpu(),x_recon_hat[0,:,2].cpu(),'b*')\n",
    "ax.plot3D(x_ahead_hat[0,:,0].cpu(),x_ahead_hat[0,:,1].cpu(),x_ahead_hat[0,:,2].cpu(),'rx')\n",
    "ax.set_xlabel('$X$', fontsize=20)\n",
    "ax.set_ylabel('$Y$',fontsize=20)\n",
    "ax.set_zlabel(r'$Z$', fontsize=20)\n",
    "plt.legend([\"Actual\",\"Reconstruction\",\"Forecasted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1830b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "%matplotlib qt5\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(x[n,horizon:,0],x[n,horizon:,1],x[n,horizon:,2],c=np.linspace(0,1,Time_Length-horizon))\n",
    "ax.scatter3D(x_ahead_hat[n,:,0].cpu(),x_ahead_hat[n,:,1].cpu(),x_ahead_hat[n,:,2].cpu())\n",
    "ax.set_xlabel('$X$', fontsize=20)\n",
    "ax.set_ylabel('$Y$',fontsize=20)\n",
    "ax.set_zlabel(r'$Z$', fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DYAN",
   "language": "python",
   "name": "dyan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
